# SVARAG: Sentiment-Aware Voice-Activated Retrieval-Augmented Generation Chatbot

![Python](https://img.shields.io/badge/Python-3.12%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-Framework-purple)
![Streamlit](https://img.shields.io/badge/Streamlit-Framework-red)
![LLM](https://img.shields.io/badge/LLM-Mistral%207B-orange)
![LLM](https://img.shields.io/badge/LLM-Llama%203-yellow)
![Status](https://img.shields.io/badge/Status-Active-success)

**SVARAG** (Sentiment-Aware Voice-Activated Retrieval-Augmented Generation Chatbot) is an advanced AI system designed to process voice queries, analyze sentiment, retrieve relevant information, and generate responses with an appropriate tone. By integrating speech recognition, sentiment analysis, retrieval-augmented generation (RAG), and text-to-speech (TTS), SVARAG provides a conversational AI experience that is both contextually aware and emotionally intelligent.

---
## ğŸ“Œ **Workflow Diagram**  
**Architecture of the system**:
![Workflow](assets/Architecture.png)

---
## ğŸ”‘ **Key Features**

ğŸ™ï¸ **Voice-Activated Query Processing**

- Users can upload or record audio to interact with the chatbot.
- Uses Whisper for speech-to-text transcription.

ğŸ§  **Multimodal Sentiment-Aware Responses**

- Extracts vocal features (pitch, volume, articulation rate) and generates natural language descriptions of the audio.
- Passes both the audio description and transcribed text to an LLM to determine the appropriate Tone Instructions for the final response.
- Ensures responses are emotionally aligned and contextually relevant based on both speech characteristics and text content.

ğŸ” **Retrieval-Augmented Generation (RAG) for Intelligent Responses**
- Retrieves relevant information from uploaded documents.
- Uses hybrid search: vector-based retrieval (ChromaDB) + keyword-based search (BM25).
- Generates factually accurate, context-aware responses using Mistral-7B LLM.

ğŸ”Š **Realistic Text-to-Speech (TTS) Output**
- Converts responses into natural-sounding speech using XTTS-v2.
- Ensures smooth, expressive voice synthesis for a conversational experience.

âš¡ **Fast & Seamless User Experience**
- Built with Streamlit for an intuitive web interface.

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```commandline
git clone https://github.com/varshath-akula/SVARAG.git
cd SVARAG
```

### 2ï¸âƒ£ Install FFmpeg (Required for Whisper)
- **Run the following commands in PowerShell (Windows users):**

  - Install Chocolatey:
    ```powershell
    Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))
    ```
  - After installing Chocolatey, use it to install ffmpeg:
    ```powershell
    choco install ffmpeg
    ```
- **Linux/Mac users can install FFmpeg using:**
```bash
sudo apt install ffmpeg  # Ubuntu/Debian  
brew install ffmpeg      # macOS (Homebrew)
```
### 3ï¸âƒ£ Create a Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate     # On Windows
```
### 4ï¸âƒ£ Install Dependencies
```commandline
pip install -r requirements.txt
```

### 5ï¸âƒ£ Set Up API Keys
Create a .env file in the root directory and add your API keys:
```
HUGGINGFACE_API_KEY = your_huggingface_key
GROQ_API_KEY = your_groq_key
```
### 6ï¸âƒ£ Change the Working Directory to `app` and Run the Streamlit App
```commandline
cd app
streamlit run app.py
```

##### âš  **Note:** While loading the **Text-to-Speech model**, you may be prompted to accept a **non-commercial license** in the terminal.
##### Type `y` and press **Enter** to proceed.


---
## ğŸ® Usage Guide
#### 1ï¸âƒ£ Upload a PDF document â†’ SVARAG processes it for retrieval.
#### 2ï¸âƒ£ Choose a query mode:
- Upload a WAV file
- Record a voice query directly
#### 3ï¸âƒ£ SVARAG processes your query:
- Transcribes speech
- Analyzes sentiment
- Retrieves relevant information
- Generates a contextually aware response
- Converts the response into speech
#### 4ï¸âƒ£ Listen to the AI-generated response ğŸ§

---
## ğŸ“š References  

This project utilizes a **multimodal sentiment analysis** approach inspired by the following research paper:  

ğŸ“„ **Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**  
âœï¸ **Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg**  
ğŸ›ï¸ **Department of Computer Science, Columbia University**  
ğŸ“§ {zw2804, zg2272, la2734, ps3391, kd2939}@columbia.edu, julia@cs.columbia.edu  

ğŸ”— [Link to the Research Paper] (https://arxiv.org/abs/2407.21315)  

This paper explores **how vocal nuances (pitch, intensity, articulation rate, etc.) can enhance LLM-based emotion recognition**. Our sentiment analysis module adopts similar methodologies, converting vocal features into **natural language descriptions** to guide the **LLM in generating sentiment-aware responses**.
